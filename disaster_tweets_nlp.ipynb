{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59fbaa2",
   "metadata": {},
   "source": [
    "# Saimon- Current Working Version\n",
    "# Natural Language Processing with DistilBERT for Disaster Tweet Classification\n",
    "\n",
    "## Project Overview\n",
    "In this project, we'll build a machine learning model that can identify whether a tweet is about a real disaster or not. This is an important application of NLP that could help emergency responders identify real emergencies from social media data.\n",
    "\n",
    "### Dataset\n",
    "- 10,000 hand-classified tweets\n",
    "- Binary classification task (disaster vs. non-disaster)\n",
    "- Features include tweet text, keywords, and location information\n",
    "\n",
    "### Technical Approach\n",
    "We're using **DistilBERT**, a lightweight and efficient transformer model:\n",
    "- 40% smaller than BERT\n",
    "- 60% faster processing\n",
    "- Maintains 97% of BERT's performance\n",
    "- Perfect for real-world applications where efficiency matters\n",
    "\n",
    "### What is BERT?\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) revolutionized NLP by:\n",
    "- Processing text bidirectionally (understanding context from both directions)\n",
    "- Pre-training on massive text datasets\n",
    "- Providing rich contextual word embeddings\n",
    "- Supporting fine-tuning for specific tasks\n",
    "\n",
    "![BERT Architecture](https://www.cse.chalmers.se/~richajo/nlp2019/l5/bert_class.png)\n",
    "\n",
    "### Notebook Contents\n",
    "1. Data Loading & Exploration\n",
    "2. Text Preprocessing\n",
    "3. Model Setup (DistilBERT)\n",
    "4. Training Pipeline\n",
    "5. Evaluation & Results\n",
    "6. Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40860bae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     DistilBertTokenizerFast,\n\u001b[0;32m     11\u001b[0m     TFDistilBertForSequenceClassification,  \n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple, Union\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     TFBaseModelOutput,\n\u001b[0;32m     30\u001b[0m     TFMaskedLMOutput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cancellation\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m executor\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_conversion_registry\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\framework\\dtypes.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_bfloat16\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cpp_shape_inference_pb2\n\u001b[1;32m---> 38\u001b[0m _np_bfloat16 \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_bfloat16\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m _np_float8_e4m3fn \u001b[38;5;241m=\u001b[39m _pywrap_float8\u001b[38;5;241m.\u001b[39mTF_float8_e4m3fn_type()\n\u001b[0;32m     40\u001b[0m _np_float8_e5m2 \u001b[38;5;241m=\u001b[39m _pywrap_float8\u001b[38;5;241m.\u001b[39mTF_float8_e5m2_type()\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    TFDistilBertForSequenceClassification,  \n",
    ")\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b000d5a",
   "metadata": {},
   "source": [
    "# Data Loading and Initial Exploration\n",
    "\n",
    "Our dataset contains tweets that have been manually classified. Let's understand its structure:\n",
    "\n",
    "### Features\n",
    "1. **id**: Unique identifier for each tweet\n",
    "2. **keyword**: A keyword extracted from the tweet (may be empty)\n",
    "3. **location**: The location the tweet was sent from (may be empty)\n",
    "4. **text**: The actual content of the tweet\n",
    "5. **target**: Our label\n",
    "   - 1 = Real disaster tweet\n",
    "   - 0 = Not a real disaster tweet\n",
    "\n",
    "### Why these features matter:\n",
    "- **keywords**: Often indicate the topic/emergency type\n",
    "- **location**: Could show geographical patterns in disaster reporting\n",
    "- **text**: Contains the main information we'll use for classification\n",
    "\n",
    "We'll load both training and test sets to begin our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a966d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"Data/train.csv\")\n",
    "df_test = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "print('Training Set Shape = {}'.format(df_train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3eda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab19753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\n",
    "df_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n",
    "\n",
    "print(\"Train Length Stat\")\n",
    "print(df_train[\"length\"].describe())\n",
    "print()\n",
    "\n",
    "print(\"Test Length Stat\")\n",
    "print(df_test[\"length\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ec6f9",
   "metadata": {},
   "source": [
    "If you want to know more information about the data, you can grab useful information [here](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n",
    "\n",
    "Note that all the tweets are in english."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d07374",
   "metadata": {},
   "source": [
    "# Meta Features Analysis\n",
    "\n",
    "Let's extract meta features that could help identify disaster tweets:\n",
    "- word_count: number of words in text\n",
    "- unique_word_count: number of unique words in text\n",
    "- stop_word_count: number of stop words in text\n",
    "- url_count: number of urls in text\n",
    "- mean_word_length: average character count in words\n",
    "- char_count: number of characters in text\n",
    "- punctuation_count: number of punctuations in text\n",
    "- hashtag_count: number of hashtags (#) in text\n",
    "- mention_count: number of mentions (@) in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "def extract_meta_features(df):\n",
    "    \"\"\"Extract meta features from the text column of a dataframe.\"\"\"\n",
    "    \n",
    "    # Get stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Create meta features\n",
    "    meta_features = pd.DataFrame()\n",
    "    \n",
    "    # Basic word counts\n",
    "    meta_features['word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    # Unique word counts\n",
    "    meta_features['unique_word_count'] = df['text'].apply(lambda x: len(set(str(x).lower().split())))\n",
    "    \n",
    "    # Stop word count\n",
    "    meta_features['stop_word_count'] = df['text'].apply(lambda x: len([word for word in str(x).lower().split() if word in stop_words]))\n",
    "    \n",
    "    # URL count\n",
    "    meta_features['url_count'] = df['text'].str.count(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    # Mean word length\n",
    "    meta_features['mean_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\n",
    "    \n",
    "    # Character count\n",
    "    meta_features['char_count'] = df['text'].str.len()\n",
    "    \n",
    "    # Punctuation count\n",
    "    meta_features['punctuation_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    \n",
    "    # Hashtag count\n",
    "    meta_features['hashtag_count'] = df['text'].str.count('#')\n",
    "    \n",
    "    # Mention count\n",
    "    meta_features['mention_count'] = df['text'].str.count('@')\n",
    "    \n",
    "    return meta_features\n",
    "\n",
    "# Extract meta features for both train and test sets\n",
    "print(\"Extracting meta features...\")\n",
    "train_meta = extract_meta_features(df_train)\n",
    "test_meta = extract_meta_features(df_test)\n",
    "\n",
    "# Add meta features to the original dataframes\n",
    "for column in train_meta.columns:\n",
    "    df_train[column] = train_meta[column]\n",
    "    df_test[column] = test_meta[column]\n",
    "\n",
    "# Display summary statistics of meta features\n",
    "print(\"\\nMeta Features Summary (Training Set):\")\n",
    "print(train_meta.describe())\n",
    "\n",
    "# Visualize meta feature distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, column in enumerate(train_meta.columns, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    \n",
    "    if df_train['target'].nunique() > 1:  # Only for training set\n",
    "        for target in [0, 1]:\n",
    "            data = df_train[df_train['target'] == target][column]\n",
    "            plt.hist(data, bins=30, alpha=0.5, label=f\"{'Disaster' if target == 1 else 'Non-Disaster'}\")\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.title(f'{column} Distribution')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf3636",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95ee47",
   "metadata": {},
   "source": [
    "Function to clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df74c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Handle empty/missing text\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs (more comprehensive pattern)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Extract hashtag content but remove the # symbol\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove numbers but keep important ones like \"911\"\n",
    "    text = re.sub(r'\\b\\d+\\b(?<!911)', '', text)\n",
    "    \n",
    "    # Remove RT (retweet) indicators\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove punctuation except ! and ? as they might indicate urgency\n",
    "    text = re.sub(r'[^\\w\\s!?]', '', text)\n",
    "    \n",
    "    # Remove repeated characters (e.g., 'helpppp' → 'help')\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "    # Strip extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87f077",
   "metadata": {},
   "source": [
    "Get all words from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4db43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disaster tweets\n",
    "disaster_tweets = df_train[df_train['target'] == 1]['text'].apply(clean_text)\n",
    "disaster_words = ' '.join(disaster_tweets).split()\n",
    "\n",
    "# Non-disaster tweets\n",
    "non_disaster_tweets = df_train[df_train['target'] == 0]['text'].apply(clean_text)\n",
    "non_disaster_words = ' '.join(non_disaster_tweets).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5abeaa",
   "metadata": {},
   "source": [
    "Saimon: The following needs to be added on EDA or after tokenization \n",
    "1. Identify parts of speech and filter\n",
    "2. Identify libraries that have some dictionary basis of words and emotional attachment, bring it over, compare tokenization to compare.\n",
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b1764",
   "metadata": {},
   "source": [
    "Create word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Disaster tweets word cloud\n",
    "wordcloud_disaster = WordCloud(width=800, height=400, background_color='white').generate(' '.join(disaster_words))\n",
    "ax1.imshow(wordcloud_disaster)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Most Common Words in Disaster Tweets')\n",
    "\n",
    "# Non-disaster tweets word cloud\n",
    "wordcloud_non_disaster = WordCloud(width=800, height=400, background_color='white').generate(' '.join(non_disaster_words))\n",
    "ax2.imshow(wordcloud_non_disaster)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Most Common Words in Non-Disaster Tweets')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66612b68",
   "metadata": {},
   "source": [
    "Show the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20 words and their counts\n",
    "disaster_top20 = Counter(disaster_words).most_common(20)\n",
    "non_disaster_top20 = Counter(non_disaster_words).most_common(20)\n",
    "\n",
    "# Convert to lists for plotting\n",
    "disaster_words_list, disaster_counts = zip(*disaster_top20)\n",
    "non_disaster_words_list, non_disaster_counts = zip(*non_disaster_top20)\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot disaster words\n",
    "bars1 = ax1.barh(range(len(disaster_words_list)), disaster_counts, color='red', alpha=0.6)\n",
    "ax1.set_yticks(range(len(disaster_words_list)))\n",
    "ax1.set_yticklabels(disaster_words_list)\n",
    "ax1.set_title('Top 20 Words in Disaster Tweets', pad=20)\n",
    "ax1.set_xlabel('Frequency')\n",
    "\n",
    "# Add count labels on the bars\n",
    "for i, bar in enumerate(bars1):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 1, bar.get_y() + bar.get_height()/2,\n",
    "             f'{int(width):,}',\n",
    "             ha='left', va='center')\n",
    "\n",
    "# Plot non-disaster words\n",
    "bars2 = ax2.barh(range(len(non_disaster_words_list)), non_disaster_counts, color='blue', alpha=0.6)\n",
    "ax2.set_yticks(range(len(non_disaster_words_list)))\n",
    "ax2.set_yticklabels(non_disaster_words_list)\n",
    "ax2.set_title('Top 20 Words in Non-Disaster Tweets', pad=20)\n",
    "ax2.set_xlabel('Frequency')\n",
    "\n",
    "# Add count labels on the bars\n",
    "for i, bar in enumerate(bars2):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width + 1, bar.get_y() + bar.get_height()/2,\n",
    "             f'{int(width):,}',\n",
    "             ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the actual counts for reference\n",
    "print(\"\\nTop 20 words in Disaster Tweets:\")\n",
    "for word, count in disaster_top20:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 20 words in Non-Disaster Tweets:\")\n",
    "for word, count in non_disaster_top20:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2640884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison plot for words that appear in both sets\n",
    "disaster_dict = dict(disaster_top20)\n",
    "non_disaster_dict = dict(non_disaster_top20)\n",
    "\n",
    "# Find common words\n",
    "common_words = set(disaster_dict.keys()) & set(non_disaster_dict.keys())\n",
    "\n",
    "if common_words:\n",
    "    # Create lists for plotting\n",
    "    common_words = list(common_words)\n",
    "    disaster_freq = [disaster_dict[word] for word in common_words]\n",
    "    non_disaster_freq = [non_disaster_dict[word] for word in common_words]\n",
    "\n",
    "    # Create comparison plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = range(len(common_words))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar([i - width/2 for i in x], disaster_freq, width, label='Disaster', color='red', alpha=0.6)\n",
    "    plt.bar([i + width/2 for i in x], non_disaster_freq, width, label='Non-Disaster', color='blue', alpha=0.6)\n",
    "\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Comparison of Common Words Between Disaster and Non-Disaster Tweets')\n",
    "    plt.xticks(x, common_words, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print comparison\n",
    "    print(\"\\nFrequency comparison of common words:\")\n",
    "    for word in common_words:\n",
    "        print(f\"{word}: Disaster={disaster_dict[word]}, Non-Disaster={non_disaster_dict[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize word frequency ratios\n",
    "if common_words:\n",
    "    ratios = [(word, disaster_dict[word] / non_disaster_dict[word]) for word in common_words]\n",
    "    ratios.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Plot ratios\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    words, ratio_values = zip(*ratios)\n",
    "    \n",
    "    plt.bar(range(len(words)), ratio_values, alpha=0.6)\n",
    "    plt.axhline(y=1, color='r', linestyle='--', alpha=0.3)\n",
    "    plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
    "    plt.ylabel('Disaster/Non-Disaster Frequency Ratio')\n",
    "    plt.title('Word Frequency Ratios (Disaster vs Non-Disaster)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nWord frequency ratios (Disaster/Non-Disaster):\")\n",
    "    for word, ratio in ratios:\n",
    "        print(f\"{word}: {ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c202c4",
   "metadata": {},
   "source": [
    "Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword analysis\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Top keywords in disaster tweets\n",
    "disaster_keywords = df_train[df_train['target'] == 1]['keyword'].value_counts().head(15)\n",
    "plt.subplot(1, 2, 1)\n",
    "disaster_keywords.plot(kind='barh')\n",
    "plt.title('Top Keywords in Disaster Tweets')\n",
    "\n",
    "# Top keywords in non-disaster tweets\n",
    "non_disaster_keywords = df_train[df_train['target'] == 0]['keyword'].value_counts().head(15)\n",
    "plt.subplot(1, 2, 2)\n",
    "non_disaster_keywords.plot(kind='barh')\n",
    "plt.title('Top Keywords in Non-Disaster Tweets')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198423d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location analysis\n",
    "print(\"\\nLocation Statistics:\")\n",
    "print(f\"Training set tweets with location: {(df_train['location'].notna().sum() / len(df_train)) * 100:.2f}%\")\n",
    "print(f\"Test set tweets with location: {(df_test['location'].notna().sum() / len(df_test)) * 100:.2f}%\")\n",
    "\n",
    "# Top locations\n",
    "print(\"\\nTop 10 Locations in Training Set:\")\n",
    "print(df_train['location'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33af53",
   "metadata": {},
   "source": [
    "Comparative Analysis between Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add word_count columns if missing\n",
    "df_train[\"word_count\"] = df_train[\"text\"].str.split().str.len()\n",
    "df_test[\"word_count\"] = df_test[\"text\"].str.split().str.len()\n",
    "\n",
    "# Compare text length distributions\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_train['length'], bins=50, alpha=0.5, label='Train')\n",
    "plt.hist(df_test['length'], bins=50, alpha=0.5, label='Test')\n",
    "plt.title('Tweet Length Distribution Comparison')\n",
    "plt.xlabel('Character Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_train['word_count'], bins=50, alpha=0.5, label='Train')\n",
    "plt.hist(df_test['word_count'], bins=50, alpha=0.5, label='Test')\n",
    "plt.title('Word Count Distribution Comparison')\n",
    "plt.xlabel('Word Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare keyword coverage\n",
    "train_keywords = set(df_train['keyword'].dropna())\n",
    "test_keywords = set(df_test['keyword'].dropna())\n",
    "\n",
    "print(\"\\nKeyword Coverage Analysis:\")\n",
    "print(f\"Unique keywords in training set: {len(train_keywords)}\")\n",
    "print(f\"Unique keywords in test set: {len(test_keywords)}\")\n",
    "print(f\"Keywords in test but not in train: {len(test_keywords - train_keywords)}\")\n",
    "print(f\"Percentage of test keywords covered by training: {(len(test_keywords.intersection(train_keywords)) / len(test_keywords)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf94be",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Model Setup\n",
    "\n",
    "### Hyperparameters\n",
    "- **BATCH_SIZE = 32**: Standard batch size for transformer models\n",
    "- **VAL_SPLIT = 0.2**: 80% training, 20% validation split\n",
    "- **MAX_LENGTH = 160**: Maximum tweet length (in tokens)\n",
    "- **MODEL_NAME = \"distilbert-base-uncased\"**: Pre-trained model we'll fine-tune\n",
    "\n",
    "### Data Processing Pipeline\n",
    "1. Split data into train/validation sets\n",
    "2. Convert to HuggingFace Datasets format\n",
    "3. Tokenize text using DistilBERT tokenizer\n",
    "4. Create PyTorch DataLoaders\n",
    "\n",
    "The tokenization process converts text into numbers that the model can understand:\n",
    "- Splits text into tokens (subwords)\n",
    "- Adds special tokens ([CLS], [SEP])\n",
    "- Pads sequences to same length\n",
    "- Creates attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf94be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic Dataset Statistics\n",
    "\n",
    "# Class distribution in training set\n",
    "print(\"Training Set Class Distribution:\")\n",
    "print(df_train[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"\\nMissing Values in Training Set:\")\n",
    "print(df_train.isnull().sum())\n",
    "print(\"\\nMissing Values in Test Set:\")\n",
    "print(df_test.isnull().sum())\n",
    "\n",
    "# Text statistics\n",
    "df_train[\"word_count\"] = df_train[\"text\"].str.split().str.len()\n",
    "df_test[\"word_count\"] = df_test[\"text\"].str.split().str.len()\n",
    "\n",
    "print(\"\\nText Statistics (Training Set):\")\n",
    "print(df_train[[\"length\", \"word_count\"]].describe())\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_train[\"length\"], bins=50)\n",
    "plt.title(\"Distribution of Tweet Lengths (Training)\")\n",
    "plt.xlabel(\"Character Count\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_train[\"word_count\"], bins=50)\n",
    "plt.title(\"Distribution of Word Counts (Training)\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT  = 0.2\n",
    "MAX_LENGTH = 160\n",
    "MODEL_NAME = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# stratified split\n",
    "train_df, val_df = train_test_split(\n",
    "    df_train,\n",
    "    test_size=VAL_SPLIT,\n",
    "    random_state=42,\n",
    "    stratify=df_train[\"target\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f696c",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "### Training Configuration\n",
    "- **Model**: DistilBERT with binary classification head\n",
    "- **Loss**: CrossEntropyLoss (standard for classification)\n",
    "- **Optimizer**: AdamW with learning rate 1e-5\n",
    "- **Epochs**: 2 full passes through the training data\n",
    "\n",
    "### Training Process\n",
    "For each epoch, we:\n",
    "1. Set model to training mode\n",
    "2. Process mini-batches of data\n",
    "3. Compute loss and gradients\n",
    "4. Update model parameters\n",
    "5. Track and display average loss\n",
    "\n",
    "### What to Watch For\n",
    "- Decreasing loss values indicate learning\n",
    "- Too rapid decrease might suggest overfitting\n",
    "- Stable loss suggests good learning rate\n",
    "\n",
    "The training loop includes proper handling of:\n",
    "- GPU acceleration (if available)\n",
    "- Gradient computation and updates\n",
    "- Batch processing\n",
    "- Loss tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0925879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap pandas DataFrames in HF Dataset\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds   = Dataset.from_pandas(val_df)\n",
    "test_ds  = Dataset.from_pandas(df_test)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "# apply tokenization\n",
    "train_ds = train_ds.map(tokenize_batch, batched=True)\n",
    "val_ds   = val_ds.map(tokenize_batch, batched=True)\n",
    "test_ds  = test_ds.map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train columns:\", train_ds.column_names)\n",
    "print(\"Train columns:\", val_ds.column_names)\n",
    "print(\"Train columns:\", test_ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b129f",
   "metadata": {},
   "source": [
    "# Rename, Format & Build DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c245876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we’d like to drop\n",
    "drop_cols = [\"__index_level_0__\", \"length\"]\n",
    "\n",
    "# Remove from train & val\n",
    "train_ds = train_ds.remove_columns([c for c in drop_cols if c in train_ds.column_names])\n",
    "val_ds   = val_ds.remove_columns(  [c for c in drop_cols if c in val_ds.column_names])\n",
    "test_ds  = test_ds.remove_columns( [c for c in drop_cols if c in test_ds.column_names])\n",
    "\n",
    "# Rename 'target' → 'labels' (only train/val have it)\n",
    "if \"target\" in train_ds.column_names:\n",
    "    train_ds = train_ds.rename_column(\"target\", \"labels\")\n",
    "if \"target\" in val_ds.column_names:\n",
    "    val_ds   = val_ds.rename_column(\"target\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ba29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch_features(batch):\n",
    "    return {\n",
    "        'input_ids': torch.tensor(batch['input_ids']),\n",
    "        'attention_mask': torch.tensor(batch['attention_mask']),\n",
    "        'labels': torch.tensor(batch['labels']) if 'labels' in batch else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b42db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Convert features to PyTorch format and select only the columns we need\n",
    "train_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_ds.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Simple collate function\n",
    "def collate_fn(examples):\n",
    "    input_ids = torch.stack([x['input_ids'] for x in examples])\n",
    "    attention_mask = torch.stack([x['attention_mask'] for x in examples])\n",
    "    if 'labels' in examples[0]:\n",
    "        labels = torch.tensor([x['labels'] for x in examples])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Temporarily disable multiprocessing\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2c756",
   "metadata": {},
   "source": [
    "# Steps per Epoch (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_loader)\n",
    "print(\"Batches per epoch:\", steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c22d3",
   "metadata": {},
   "source": [
    "# Load a DistilBERT model from Keras NLP\n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n",
    "\n",
    "The BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n",
    "\n",
    "We will choose DistilBERT model.that learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters ([paper](https://arxiv.org/abs/1910.01108)). \n",
    "\n",
    "It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n",
    "\n",
    "Specifically, it doesn't have token-type embeddings, pooler and retains only half of the layers from Google's BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "# 1) Tokenizer (handles tokenization + attention mask)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2) Pretrained classifier head on top of DistilBERT\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,            # your binary task\n",
    ")\n",
    "\n",
    "# 3) Try to use DirectML for AMD GPU, fallback to CPU if not available\n",
    "try:\n",
    "    import torch_directml\n",
    "    dml = torch_directml.device()\n",
    "    print(f\"Using DirectML device: {dml}\")\n",
    "    device = dml\n",
    "except Exception:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Falling back to CPU: {device}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ce007",
   "metadata": {},
   "source": [
    "# Setup optimizer & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a08f84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Binary classification → use CrossEntropyLoss (expects raw logits)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# AdamW is standard for Transformers, but disable foreach for DirectML compatibility\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-5,\n",
    "    foreach=False  # Disable foreach path to avoid CPU fallback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7d031",
   "metadata": {},
   "source": [
    "# Fine-tune DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf686f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 2\n",
    "\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for batch in train_loader:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             labels=labels\n",
    "#         )\n",
    "        \n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     print(f\"Epoch {epoch}/{EPOCHS} — loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding modified fine-tuning \n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 2\n",
    "best_f1 = 0.0\n",
    "\n",
    "# Initialize scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=EPOCHS * len(train_loader)\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} — loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    true_labels, pred_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
    "    val_acc = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "    print(f\"Validation — F1: {val_f1:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(f\"Best model saved (F1: {best_f1:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41a635",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "### Evaluation Metrics\n",
    "We use several metrics to assess model performance:\n",
    "\n",
    "1. **Confusion Matrix**\n",
    "   - Shows True Positives, False Positives, True Negatives, False Negatives\n",
    "   - Helps visualize where model makes mistakes\n",
    "\n",
    "2. **F1 Score**\n",
    "   - Harmonic mean of precision and recall\n",
    "   - Balances false positives and false negatives\n",
    "   - Particularly important for imbalanced datasets\n",
    "\n",
    "### Prediction Process\n",
    "The `get_predictions` function:\n",
    "1. Sets model to evaluation mode\n",
    "2. Disables gradient computation\n",
    "3. Processes batches of data\n",
    "4. Returns both true labels and predicted probabilities\n",
    "\n",
    "### Visualization\n",
    "The confusion matrix plot shows:\n",
    "- Correct predictions on diagonal\n",
    "- Errors off-diagonal\n",
    "- Color intensity indicates frequency\n",
    "- Labels for easy interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.append(probs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        np.concatenate(all_labels, axis=0),\n",
    "        np.concatenate(all_preds, axis=0),\n",
    "    )\n",
    "\n",
    "\n",
    "def display_confusion_matrix(y_true, y_pred_probs, dataset_name):\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Disaster\", \"Disaster\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix — {dataset_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # F1 score from sklearn instead of manual formula (more reliable)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"{dataset_name} F1 score: {f1:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0859da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, p_train = get_predictions(train_loader)\n",
    "display_confusion_matrix(y_train, p_train, \"Training\")\n",
    "\n",
    "y_val, p_val = get_predictions(val_loader)\n",
    "display_confusion_matrix(y_val, p_val, \"Validation\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
