{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation and Prediction\n",
    "\n",
    "This notebook performs final model evaluation, generates test predictions, and creates the submission file.\n",
    "\n",
    "## Steps:\n",
    "1. Load trained model and test data\n",
    "2. Generate predictions on test set\n",
    "3. Create submission file\n",
    "4. Comprehensive performance analysis\n",
    "5. Generate visualizations and reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model architecture (same as training notebook)\n",
    "class HybridDistilBERTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid model combining DistilBERT with meta-features.\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_model_name, num_meta_features, num_labels=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # DistilBERT model\n",
    "        self.bert = DistilBertModel.from_pretrained(bert_model_name)\n",
    "        self.bert_dim = self.bert.config.hidden_size\n",
    "        \n",
    "        # Meta-feature processing\n",
    "        self.meta_bn = nn.BatchNorm1d(num_meta_features)\n",
    "        self.meta_fc = nn.Linear(num_meta_features, 32)\n",
    "        self.meta_activation = nn.ReLU()\n",
    "        self.meta_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Combined features\n",
    "        combined_dim = self.bert_dim + 32\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, meta_features):\n",
    "        # BERT encoding\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_pooled = bert_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Meta-feature processing\n",
    "        meta_processed = self.meta_bn(meta_features)\n",
    "        meta_processed = self.meta_fc(meta_processed)\n",
    "        meta_processed = self.meta_activation(meta_processed)\n",
    "        meta_processed = self.meta_dropout(meta_processed)\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([bert_pooled, meta_processed], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(combined)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Load trained model\n",
    "checkpoint = torch.load('models/best_model.pth', map_location=device)\n",
    "model_info = checkpoint.get('config', {})\n",
    "\n",
    "# Recreate model\n",
    "model = HybridDistilBERTClassifier(\n",
    "    bert_model_name=model_info.get('model_config', {}).get('model_name', 'distilbert-base-uncased'),\n",
    "    num_meta_features=10,  # Based on our feature engineering\n",
    "    num_labels=2,\n",
    "    dropout=model_info.get('model_config', {}).get('dropout', 0.2)\n",
    ")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "print(f\"Best validation F1: {checkpoint.get('val_f1', 'N/A')}\")\n",
    "print(f\"Best epoch: {checkpoint.get('epoch', 'N/A') + 1}\")\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_csv('Data/test_cleaned.csv')\n",
    "print(f\"\\nTest data shape: {df_test.shape}\")\n",
    "\n",
    "# Identify meta-feature columns\n",
    "meta_cols = [col for col in df_test.columns if col.endswith('_meta')]\n",
    "print(f\"Meta-features found: {len(meta_cols)}\")\n",
    "print(f\"Meta-feature columns: {meta_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset class from training notebook\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Custom Dataset class\n",
    "class DisasterTweetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for combining BERT tokens with meta-features.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, meta_features, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.meta_features = torch.FloatTensor(meta_features)\n",
    "        self.labels = torch.LongTensor(labels) if labels is not None else None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.meta_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'meta_features': self.meta_features[idx]\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Prepare test data\n",
    "test_texts = df_test['text_clean'].values\n",
    "test_meta = df_test[meta_cols].values\n",
    "test_ids = df_test['id'].values\n",
    "\n",
    "# Tokenize test texts\n",
    "print(\"Tokenizing test data...\")\n",
    "test_encodings = tokenizer(\n",
    "    test_texts.tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=160,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = DisasterTweetDataset(test_encodings, test_meta)\n",
    "\n",
    "# Create test dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Verify data integrity\n",
    "print(f\"\\nTest data shapes:\")\n",
    "print(f\"Texts: {test_texts.shape}\")\n",
    "print(f\"Meta features: {test_meta.shape}\")\n",
    "print(f\"IDs: {test_ids.shape}\")\n",
    "print(f\"Token encodings: {test_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"Generating test predictions...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        # Move inputs to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        meta_features = batch['meta_features'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask, meta_features)\n",
    "        \n",
    "        # Get probabilities and predictions\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "predictions = np.array(all_predictions)\n",
    "probabilities = np.array(all_probabilities)\n",
    "\n",
    "print(f\"\\nâœ… Predictions generated!\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "\n",
    "# Prediction distribution\n",
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    label = 'Non-Disaster' if cls == 0 else 'Disaster'\n",
    "    percentage = count / len(predictions) * 100\n",
    "    print(f\"{label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Confidence analysis\n",
    "confidence_scores = np.max(probabilities, axis=1)\n",
    "print(f\"\\nConfidence statistics:\")\n",
    "print(f\"Mean confidence: {confidence_scores.mean():.4f}\")\n",
    "print(f\"Min confidence: {confidence_scores.min():.4f}\")\n",
    "print(f\"Max confidence: {confidence_scores.max():.4f}\")\n",
    "print(f\"Std confidence: {confidence_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'target': predictions\n",
    "})\n",
    "\n",
    "# Add confidence scores\n",
    "submission_df['confidence'] = confidence_scores\n",
    "submission_df['disaster_probability'] = probabilities[:, 1]  # Probability of disaster class\n",
    "\n",
    "# Display sample of submission\n",
    "print(\"Sample submission:\")\n",
    "display(submission_df.head(10))\n",
    "\n",
    "# Verify submission format\n",
    "print(f\"\\nSubmission verification:\")\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(f\"ID range: {submission_df['id'].min()} to {submission_df['id'].max()}\")\n",
    "print(f\"Target values: {sorted(submission_df['target'].unique())}\")\n",
    "print(f\"Missing values: {submission_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Compare with expected format\n",
    "expected_test_size = len(df_test)\n",
    "actual_test_size = len(submission_df)\n",
    "print(f\"Expected test samples: {expected_test_size}\")\n",
    "print(f\"Actual predictions: {actual_test_size}\")\n",
    "print(f\"Match: {expected_test_size == actual_test_size}\")\n",
    "\n",
    "# Save submission file\n",
    "submission_path = 'Data/submission.csv'\n",
    "submission_df[['id', 'target']].to_csv(submission_path, index=False)\n",
    "\n",
    "# Save enhanced submission with confidence scores\n",
    "enhanced_submission_path = 'Data/submission_enhanced.csv'\n",
    "submission_df.to_csv(enhanced_submission_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Submission files created:\")\n",
    "print(f\"- {submission_path} (for competition)\")\n",
    "print(f\"- {enhanced_submission_path} (with confidence scores)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Load Validation Data for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data for validation analysis\n",
    "df_train = pd.read_csv('Data/train_cleaned.csv')\n",
    "\n",
    "# Prepare validation data (using the same split as training)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "meta_cols = [col for col in df_train.columns if col.endswith('_meta')]\n",
    "X_text = df_train['text_clean'].values\n",
    "y = df_train['target'].values\n",
    "X_meta = df_train[meta_cols].values\n",
    "\n",
    "# Split to get validation set\n",
    "X_text_train, X_text_val, X_meta_train, X_meta_val, y_train, y_val = train_test_split(\n",
    "    X_text, X_meta, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Tokenize validation data\n",
    "val_encodings = tokenizer(\n",
    "    X_text_val.tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=160,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "val_dataset = DisasterTweetDataset(val_encodings, X_meta_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Generate validation predictions\n",
    "print(\"Generating validation predictions for analysis...\")\n",
    "\n",
    "val_predictions = []\n",
    "val_probabilities = []\n",
    "val_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        # Move inputs to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        meta_features = batch['meta_features'].to(device)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask, meta_features)\n",
    "        \n",
    "        # Get probabilities and predictions\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        val_predictions.extend(predictions.cpu().numpy())\n",
    "        val_probabilities.extend(probabilities.cpu().numpy())\n",
    "        val_labels_list.extend(labels.numpy())\n",
    "\n",
    "val_predictions = np.array(val_predictions)\n",
    "val_probabilities = np.array(val_probabilities)\n",
    "val_labels = np.array(val_labels_list)\n",
    "\n",
    "print(f\"\\nâœ… Validation predictions generated!\")\n",
    "print(f\"Validation samples: {len(val_labels)}\")\n",
    "\n",
    "# Calculate validation metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "val_f1 = f1_score(val_labels, val_predictions, average='weighted')\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "val_precision = precision_score(val_labels, val_predictions, average='weighted')\n",
    "val_recall = recall_score(val_labels, val_predictions, average='weighted')\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation Performance:\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1 Score: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Comprehensive Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Detailed Classification Report (Validation):\")\n",
    "print(\"=\"*60)\n",
    "report = classification_report(\n",
    "    val_labels, val_predictions, \n",
    "    target_names=['Non-Disaster', 'Disaster'],\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Print formatted report\n",
    "print(f\"{'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for class_name in ['Non-Disaster', 'Disaster']:\n",
    "    metrics = report[class_name]\n",
    "    print(f\"{class_name:<15} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} {metrics['f1-score']:<10.4f} {metrics['support']:<10.0f}\")\n",
    "\n",
    "print(f\"{'accuracy':<15} {report['accuracy']:<10.4f} {report['accuracy']:<10.4f} {report['accuracy']:<10.4f} {report['macro avg']['support']:<10.0f}\")\n",
    "print(f\"{'macro avg':<15} {report['macro avg']['precision']:<10.4f} {report['macro avg']['recall']:<10.4f} {report['macro avg']['f1-score']:<10.4f} {report['macro avg']['support']:<10.0f}\")\n",
    "print(f\"{'weighted avg':<15} {report['weighted avg']['precision']:<10.4f} {report['weighted avg']['recall']:<10.4f} {report['weighted avg']['f1-score']:<10.4f} {report['weighted avg']['support']:<10.0f}\")\n",
    "\n",
    "# Per-class analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Per-Class Performance Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, class_name in enumerate(['Non-Disaster', 'Disaster']):\n",
    "    class_mask = val_labels == i\n",
    "    class_preds = val_predictions[class_mask]\n",
    "    class_probs = val_probabilities[class_mask, i]\n",
    "    \n",
    "    # True positives, false positives, false negatives\n",
    "    tp = (class_preds == i).sum()\n",
    "    fp = (class_preds != i).sum()\n",
    "    fn = (val_labels[class_preds == i] != i).sum()\n",
    "    \n",
    "    # Specificity (true negative rate)\n",
    "    tn = ((val_predictions != i) & (val_labels != i)).sum()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{class_name} Class:\")\n",
    "    print(f\"  Samples: {class_mask.sum()}\")\n",
    "    print(f\"  True Positives: {tp}\")\n",
    "    print(f\"  False Positives: {fp}\")\n",
    "    print(f\"  False Negatives: {fn}\")\n",
    "    print(f\"  True Negatives: {tn}\")\n",
    "    print(f\"  Specificity: {specificity:.4f}\")\n",
    "    print(f\"  Avg Confidence: {class_probs.mean():.4f}\")\n",
    "    print(f\"  Confidence Std: {class_probs.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Advanced Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(val_labels, val_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "            xticklabels=['Non-Disaster', 'Disaster'],\n",
    "            yticklabels=['Non-Disaster', 'Disaster'])\n",
    "axes[0, 0].set_title('Confusion Matrix')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Actual')\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(val_labels, val_probabilities[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[0, 1].set_xlim([0.0, 1.0])\n",
    "axes[0, 1].set_ylim([0.0, 1.05])\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve')\n",
    "axes[0, 1].legend(loc=\"lower right\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(val_labels, val_probabilities[:, 1])\n",
    "pr_auc = average_precision_score(val_labels, val_probabilities[:, 1])\n",
    "axes[0, 2].plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "axes[0, 2].set_xlabel('Recall')\n",
    "axes[0, 2].set_ylabel('Precision')\n",
    "axes[0, 2].set_title('Precision-Recall Curve')\n",
    "axes[0, 2].legend(loc=\"lower left\")\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Confidence Distribution by Class\n",
    "confidence_correct = []\n",
    "confidence_incorrect = []\n",
    "for i, (true_label, pred_label, probs) in enumerate(zip(val_labels, val_predictions, val_probabilities)):\n",
    "    confidence = probs[pred_label]\n",
    "    if true_label == pred_label:\n",
    "        confidence_correct.append(confidence)\n",
    "    else:\n",
    "        confidence_incorrect.append(confidence)\n",
    "\n",
    "axes[1, 0].hist(confidence_correct, bins=30, alpha=0.7, label='Correct Predictions', color='green')\n",
    "axes[1, 0].hist(confidence_incorrect, bins=30, alpha=0.7, label='Incorrect Predictions', color='red')\n",
    "axes[1, 0].set_xlabel('Confidence Score')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Confidence Distribution by Prediction Correctness')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Prediction Distribution on Test Set\n",
    "test_disaster_probs = submission_df['disaster_probability']\n",
    "axes[1, 1].hist(test_disaster_probs, bins=30, alpha=0.7, color='purple')\n",
    "axes[1, 1].set_xlabel('Disaster Probability')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Test Set Prediction Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Class Performance Comparison\n",
    "classes = ['Non-Disaster', 'Disaster']\n",
    "precision_scores = [report['Non-Disaster']['precision'], report['Disaster']['precision']]\n",
    "recall_scores = [report['Non-Disaster']['recall'], report['Disaster']['recall']]\n",
    "f1_scores = [report['Non-Disaster']['f1-score'], report['Disaster']['f1-score']]\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 2].bar(x - width, precision_scores, width, label='Precision', alpha=0.8)\n",
    "axes[1, 2].bar(x, recall_scores, width, label='Recall', alpha=0.8)\n",
    "axes[1, 2].bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "axes[1, 2].set_xlabel('Class')\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].set_title('Class Performance Comparison')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(classes)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save visualizations\n",
    "plt.savefig('results/visualizations/comprehensive_performance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ… Performance visualizations saved to results/visualizations/comprehensive_performance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform error analysis\n",
    "print(\"Error Analysis on Validation Set:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find misclassified examples\n",
    "misclassified_mask = val_predictions != val_labels\n",
    "misclassified_indices = np.where(misclassified_mask)[0]\n",
    "\n",
    "print(f\"Total misclassified: {len(misclassified_indices)} out of {len(val_labels)} ({len(misclassified_indices)/len(val_labels)*100:.1f}%)\")\n",
    "\n",
    "# Analyze error types\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "\n",
    "for idx in misclassified_indices:\n",
    "    true_label = val_labels[idx]\n",
    "    pred_label = val_predictions[idx]\n",
    "    confidence = val_probabilities[idx, pred_label]\n",
    "    \n",
    "    if true_label == 0 and pred_label == 1:  # False positive\n",
    "        false_positives.append({\n",
    "            'index': idx,\n",
    "            'confidence': confidence,\n",
    "            'true_label': 'Non-Disaster',\n",
    "            'pred_label': 'Disaster'\n",
    "        })\n",
    "    elif true_label == 1 and pred_label == 0:  # False negative\n",
    "        false_negatives.append({\n",
    "            'index': idx,\n",
    "            'confidence': confidence,\n",
    "            'true_label': 'Disaster',\n",
    "            'pred_label': 'Non-Disaster'\n",
    "        })\n",
    "\n",
    "print(f\"\\nFalse Positives (Non-Disaster predicted as Disaster): {len(false_positives)}\")\n",
    "print(f\"False Negatives (Disaster predicted as Non-Disaster): {len(false_negatives)}\")\n",
    "\n",
    "# Show examples of errors\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Error Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get original texts for misclassified examples\n",
    "val_indices = np.where((np.arange(len(y_train))[:, np.newaxis] == y_train.reshape(-1, 1)).all(axis=1))[0]\n",
    "\n",
    "# Show false positives\n",
    "print(\"\\nFalse Positives (Non-Disaster â†’ Disaster):\")\n",
    "print(\"-\" * 50)\n",
    "for i, error in enumerate(false_positives[:3]):  # Show first 3\n",
    "    idx = error['index']\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Text: {X_text_val[idx][:100]}...\")\n",
    "    print(f\"  Confidence: {error['confidence']:.4f}\")\n",
    "    print(f\"  True: {error['true_label']}, Predicted: {error['pred_label']}\")\n",
    "    print()\n",
    "\n",
    "# Show false negatives\n",
    "print(\"False Negatives (Disaster â†’ Non-Disaster):\")\n",
    "print(\"-\" * 50)\n",
    "for i, error in enumerate(false_negatives[:3]):  # Show first 3\n",
    "    idx = error['index']\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Text: {X_text_val[idx][:100]}...\")\n",
    "    print(f\"  Confidence: {error['confidence']:.4f}\")\n",
    "    print(f\"  True: {error['true_label']}, Predicted: {error['pred_label']}\")\n",
    "    print()\n",
    "\n",
    "# Confidence analysis for errors\n",
    "fp_confidences = [error['confidence'] for error in false_positives]\n",
    "fn_confidences = [error['confidence'] for error in false_negatives]\n",
    "\n",
    "print(f\"\\nError Confidence Analysis:\")\n",
    "print(f\"False Positives - Mean confidence: {np.mean(fp_confidences):.4f}, Std: {np.std(fp_confidences):.4f}\")\n",
    "print(f\"False Negatives - Mean confidence: {np.mean(fn_confidences):.4f}, Std: {np.std(fn_confidences):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 Save Final Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final results\n",
    "final_results = {\n",
    "    'model_performance': {\n",
    "        'validation_accuracy': float(val_accuracy),\n",
    "        'validation_precision': float(val_precision),\n",
    "        'validation_recall': float(val_recall),\n",
    "        'validation_f1': float(val_f1),\n",
    "        'roc_auc': float(roc_auc),\n",
    "        'pr_auc': float(pr_auc)\n",
    "    },\n",
    "    'per_class_performance': {\n",
    "        'non_disaster': {\n",
    "            'precision': report['Non-Disaster']['precision'],\n",
    "            'recall': report['Non-Disaster']['recall'],\n",
    "            'f1_score': report['Non-Disaster']['f1-score'],\n",
    "            'support': report['Non-Disaster']['support']\n",
    "        },\n",
    "        'disaster': {\n",
    "            'precision': report['Disaster']['precision'],\n",
    "            'recall': report['Disaster']['recall'],\n",
    "            'f1_score': report['Disaster']['f1-score'],\n",
    "            'support': report['Disaster']['support']\n",
    "        }\n",
    "    },\n",
    "    'error_analysis': {\n",
    "        'total_errors': len(misclassified_indices),\n",
    "        'error_rate': len(misclassified_indices) / len(val_labels),\n",
    "        'false_positives': len(false_positives),\n",
    "        'false_negatives': len(false_negatives),\n",
    "        'fp_mean_confidence': float(np.mean(fp_confidences)) if fp_confidences else 0,\n",
    "        'fn_mean_confidence': float(np.mean(fn_confidences)) if fn_confidences else 0\n",
    "    },\n",
    "    'test_predictions': {\n",
    "        'total_samples': len(predictions),\n",
    "        'disaster_predictions': int(np.sum(predictions)),\n",
    "        'non_disaster_predictions': int(np.sum(predictions == 0)),\n",
    "        'disaster_percentage': float(np.mean(predictions)),\n",
    "        'mean_confidence': float(np.mean(confidence_scores)),\n",
    "        'confidence_std': float(np.std(confidence_scores))\n",
    "    },\n",
    "    'model_info': {\n",
    "        'model_type': 'Hybrid DistilBERT + Meta-Features',\n",
    "        'bert_model': 'distilbert-base-uncased',\n",
    "        'num_meta_features': len(meta_cols),\n",
    "        'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'best_epoch': int(checkpoint.get('epoch', 0)) + 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save comprehensive results\n",
    "with open('results/metrics/final_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(\"ðŸŽ¯ FINAL MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Validation Performance:\")\n",
    "print(f\"  Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "print(f\"  F1 Score: {val_f1:.4f}\")\n",
    "print(f\"  Precision: {val_precision:.4f}\")\n",
    "print(f\"  Recall: {val_recall:.4f}\")\n",
    "print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Per-Class Performance:\")\n",
    "print(f\"  Non-Disaster - F1: {report['Non-Disaster']['f1-score']:.4f}, Precision: {report['Non-Disaster']['precision']:.4f}, Recall: {report['Non-Disaster']['recall']:.4f}\")\n",
    "print(f\"  Disaster - F1: {report['Disaster']['f1-score']:.4f}, Precision: {report['Disaster']['precision']:.4f}, Recall: {report['Disaster']['recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Test Predictions:\")\n",
    "print(f\"  Total samples: {len(predictions)}\")\n",
    "print(f\"  Disaster predictions: {np.sum(predictions)} ({np.mean(predictions)*100:.1f}%)\")\n",
    "print(f\"  Mean confidence: {np.mean(confidence_scores):.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ” Error Analysis:\")\n",
    "print(f\"  Total errors: {len(misclassified_indices)} ({len(misclassified_indices)/len(val_labels)*100:.1f}%)\")\n",
    "print(f\"  False positives: {len(false_positives)}\")\n",
    "print(f\"  False negatives: {len(false_negatives)}\")\n",
    "\n",
    "print(f\"\\nðŸ¤– Model Architecture:\")\n",
    "print(f\"  Type: Hybrid DistilBERT + Meta-Features\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Meta features: {len(meta_cols)}\")\n",
    "print(f\"  Best epoch: {checkpoint.get('epoch', 0) + 1}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Files Created:\")\n",
    "print(f\"  - Data/submission.csv (competition submission)\")\n",
    "print(f\"  - Data/submission_enhanced.csv (with confidence scores)\")\n",
    "print(f\"  - results/metrics/final_results.json (comprehensive metrics)\")\n",
    "print(f\"  - results/visualizations/comprehensive_performance.png\")\n",
    "print(f\"  - models/best_model.pth (trained model)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ Disaster Tweet Classification Project Completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Next Steps and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print recommendations based on analysis\n",
    "print(\"ðŸš€ RECOMMENDATIONS FOR FUTURE IMPROVEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ”§ Model Improvements:\")\n",
    "print(\"  1. Try larger BERT models (BERT-base, RoBERTa)\")\n",
    "print(\"  2. Implement ensemble methods (multiple models)\")\n",
    "print(\"  3. Add more sophisticated meta-features\")\n",
    "print(\"  4. Experiment with different architectures (LSTM + BERT)\")\n",
    "\n",
    "print(\"\\nðŸ“Š Data Enhancements:\")\n",
    "print(\"  1. Data augmentation techniques (back-translation, synonym replacement)\")\n",
    "print(\"  2. Handle class imbalance with weighted loss or sampling\")\n",
    "print(\"  3. Collect more labeled data for minority class\")\n",
    "print(\"  4. Implement cross-validation for robust evaluation\")\n",
    "\n",
    "print(\"\\nâš¡ Training Optimizations:\")\n",
    "print(\"  1. Hyperparameter tuning (learning rate, batch size, dropout)\")\n",
    "print(\"  2. Advanced regularization techniques\")\n",
    "print(\"  3. Learning rate scheduling and warmup\")\n",
    "print(\"  4. Mixed precision training for faster training\")\n",
    "\n",
    "print(\"\\nðŸŒ Deployment Ready:\")\n",
    "print(\"  1. Create REST API for model inference\")\n",
    "print(\"  2. Implement real-time Twitter stream processing\")\n",
    "print(\"  3. Add model monitoring and drift detection\")\n",
    "print(\"  4. Containerize with Docker for easy deployment\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Based on Error Analysis:\")\")\n",
    "if len(false_negatives) > len(false_positives):\n",
    "    print(\"  - Focus on improving disaster recall (missed disasters)\")\n",
    "    print(\"  - Consider using focal loss to prioritize hard examples\")\n",
    "else:\n",
    "    print(\"  - Focus on reducing false alarms\")\n",
    "    print(\"  - Adjust classification threshold if needed\")\n",
    "\n",
    "print(f\"\\nâœ… Project successfully completed with {val_f1:.4f} F1 score!\")\n",
    "print(\"Ready for production deployment and further improvements.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}