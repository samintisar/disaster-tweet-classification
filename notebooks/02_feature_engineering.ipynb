{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering\n",
    "\n",
    "This notebook focuses on extracting advanced meta-features from tweet text to complement the DistilBERT model.\n",
    "\n",
    "## Features to Extract:\n",
    "- **Linguistic**: Word count, unique words, mean word length, character count\n",
    "- **Syntactic**: Stop word count, punctuation count\n",
    "- **Social Media**: Hashtag count, mention count, URL count\n",
    "- **Sentiment**: VADER compound sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Load configuration\n",
    "with open('../config/hyperparameters.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set up VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Enhanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enhanced datasets from previous notebook\n",
    "df_train = pd.read_csv('Data/train_enhanced.csv')\n",
    "df_test = pd.read_csv('Data/test_enhanced.csv')\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "# Verify columns\n",
    "print(f\"\\nTrain columns: {list(df_train.columns)}\")\n",
    "print(f\"Test columns: {list(df_test.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Meta-Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_meta_features(df):\n",
    "    \"\"\"\n",
    "    Extract meta-features from tweet text.\n",
    "    Returns DataFrame with 10 engineered features.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Basic text statistics\n",
    "    features['word_count'] = df['text'].str.split().str.len()\n",
    "    features['unique_word_count'] = df['text'].apply(lambda x: len(set(str(x).lower().split())))\n",
    "    features['char_count'] = df['text'].str.len()\n",
    "    features['mean_word_length'] = df['text'].apply(\n",
    "        lambda x: np.mean([len(w) for w in str(x).split()]) if str(x).split() else 0\n",
    "    )\n",
    "    \n",
    "    # Stop word analysis\n",
    "    features['stop_word_count'] = df['text'].apply(\n",
    "        lambda x: sum(1 for w in str(x).lower().split() if w in stop_words)\n",
    "    )\n",
    "    \n",
    "    # Social media features\n",
    "    features['url_count'] = df['text'].str.count(r'http[s]?://\\S+|www\\.\\S+')\n",
    "    features['hashtag_count'] = df['text'].str.count('#')\n",
    "    features['mention_count'] = df['text'].str.count('@')\n",
    "    \n",
    "    # Punctuation analysis\n",
    "    features['punctuation_count'] = df['text'].apply(\n",
    "        lambda x: sum(1 for c in str(x) if c in string.punctuation)\n",
    "    )\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    features['vader_compound'] = df['text'].apply(\n",
    "        lambda x: sia.polarity_scores(str(x))['compound']\n",
    "    )\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"✅ Feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Extract Meta-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract meta-features for training and test sets\n",
    "print(\"Extracting meta-features for training data...\")\n",
    "train_meta = extract_meta_features(df_train)\n",
    "\n",
    "print(\"Extracting meta-features for test data...\")\n",
    "test_meta = extract_meta_features(df_test)\n",
    "\n",
    "print(f\"\\nTraining meta-features shape: {train_meta.shape}\")\n",
    "print(f\"Test meta-features shape: {test_meta.shape}\")\n",
    "\n",
    "# Display sample of extracted features\n",
    "print(\"\\nSample of extracted meta-features:\")\n",
    "display(train_meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "meta
   "outputs": [],
   "source": [
    "# Basic statistics of extracted features\n",
    "print(\"Training meta-features statistics:\")\n",
    "display(train_meta.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(\"Test meta-features statistics:\")\n",
    "display(test_meta.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(train_meta.columns):\n",
    "    # Plot by class for training data\n",
    "    for target, label, color in [(0, 'Non-Disaster', 'lightcoral'), (1, 'Disaster', 'lightblue')]:\n",
    "        mask = df_train['target'] == target\n",
    "        axes[i].hist(train_meta[mask][feature], alpha=0.6, label=label, color=color, bins=30)\n",
    "    \n",
    "    axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(train_meta.columns) < len(axes):\n",
    "    for j in range(len(train_meta.columns), len(axes)):\n",
    "        axes[j].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add target to meta-features for correlation analysis\n",
    "train_meta_with_target = train_meta.copy()\n",
    "train_meta_with_target['target'] = df_train['target']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = train_meta_with_target.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with target\n",
    "target_correlation = correlation_matrix['target'].sort_values(ascending=False)\n",
    "print(\"Feature correlation with target variable:\")\n",
    "print(target_correlation.drop('target').round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Feature Comparison by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison by class\n",
    "print(\"Statistical comparison of features by class:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for feature in train_meta.columns:\n",
    "    disaster_mean = train_meta[df_train['target'] == 1][feature].mean()\n",
    "    non_disaster_mean = train_meta[df_train['target'] == 0][feature].mean()\n",
    "    difference = disaster_mean - non_disaster_mean\n",
    "    percent_diff = (difference / non_disaster_mean) * 100 if non_disaster_mean != 0 else 0\n",
    "    \n",
    "    print(f\"\\n{feature.replace('_', ' ').title():25}:\")\n",
    "    print(f\"  Disaster mean:     {disaster_mean:.3f}\")\n",
    "    print(f\"  Non-Disaster mean: {non_disaster_mean:.3f}\")\n",
    "    print(f\"  Difference:        {difference:.3f} ({percent_diff:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple statistical test to identify most discriminative features\n",
    "from scipy import stats\n",
    "\n",
    "feature_importance = []\n",
    "\n",
    "for feature in train_meta.columns:\n",
    "    disaster_values = train_meta[df_train['target'] == 1][feature]\n",
    "    non_disaster_values = train_meta[df_train['target'] == 0][feature]\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat, p_value = stats.ttest_ind(disaster_values, non_disaster_values, equal_var=False)\n",
    "    \n",
    "    # Calculate effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((disaster_values.var() + non_disaster_values.var()) / 2)\n",
    "    cohen_d = (disaster_values.mean() - non_disaster_values.mean()) / pooled_std\n",
    "    \n",
    "    feature_importance.append({\n",
    "        'feature': feature,\n",
    "        'p_value': p_value,\n",
    "        't_statistic': t_stat,\n",
    "        'effect_size': cohen_d,\n",
    "        'significant': p_value < 0.05\n",
    "    })\n",
    "\n",
    "feature_importance_df = pd.DataFrame(feature_importance)\n",
    "feature_importance_df = feature_importance_df.sort_values('p_value')\n",
    "\n",
    "print(\"Feature importance analysis:\")\n",
    "display(feature_importance_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "train_meta_scaled = scaler.fit_transform(train_meta)\n",
    "test_meta_scaled = scaler.transform(test_meta)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "train_meta_scaled_df = pd.DataFrame(train_meta_scaled, columns=train_meta.columns, index=train_meta.index)\n",
    "test_meta_scaled_df = pd.DataFrame(test_meta_scaled, columns=test_meta.columns, index=test_meta.index)\n",
    "\n",
    "print(\"Features normalized using StandardScaler.\")\n",
    "print(f\"Training meta-features scaled shape: {train_meta_scaled_df.shape}\")\n",
    "print(f\"Test meta-features scaled shape: {test_meta_scaled_df.shape}\")\n",
    "\n",
    "# Display sample of scaled features\n",
    "print(\"\\nSample of normalized features:\")\n",
    "display(train_meta_scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Integrate Features with Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalized meta-features to original DataFrames\n",
    "train_with_meta = df_train.copy()\n",
    "test_with_meta = df_test.copy()\n",
    "\n",
    "# Add normalized features with '_meta' suffix\n",
    "for col in train_meta_scaled_df.columns:\n",
    "    train_with_meta[f'{col}_meta'] = train_meta_scaled_df[col]\n",
    "    test_with_meta[f'{col}_meta'] = test_meta_scaled_df[col]\n",
    "\n",
    "print(f\"Final training dataset shape: {train_with_meta.shape}\")\n",
    "print(f\"Final test dataset shape: {test_with_meta.shape}\")\n",
    "\n",
    "print(\"\\nNew columns added:\")\n",
    "new_cols = [col for col in train_with_meta.columns if col.endswith('_meta')]\n",
    "print(new_cols)\n",
    "\n",
    "# Display sample of final dataset\n",
    "print(\"\\nSample of final training dataset:\")\n",
    "display(train_with_meta.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Save Enhanced Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets with meta-features\n",
    "train_with_meta.to_csv('Data/train_with_meta.csv', index=False)\n",
    "test_with_meta.to_csv('Data/test_with_meta.csv', index=False)\n",
    "\n",
    "# Save meta-features separately\n",
    "train_meta_scaled_df.to_csv('Data/train_meta_features.csv', index=False)\n",
    "test_meta_scaled_df.to_csv('Data/test_meta_features.csv', index=False)\n",
    "\n",
    "# Save scaler for future use\n",
    "import joblib\n",
    "joblib.dump(scaler, 'models/meta_feature_scaler.pkl')\n",
    "\n",
    "print(\"✅ Feature engineering completed!\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"- Data/train_with_meta.csv\")\n",
    "print(\"- Data/test_with_meta.csv\")\n",
    "print(\"- Data/train_meta_features.csv\")\n",
    "print(\"- Data/test_meta_features.csv\")\n",
    "print(\"- models/meta_feature_scaler.pkl\")\n",
    "\n",
    "# Save feature importance analysis\n",
    "feature_importance_df.to_csv('results/metrics/feature_importance.csv', index=False)\n",
    "print(\"- results/metrics/feature_importance.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 Feature engineering completed successfully!\")\n",
    "print(\"Next: Text Preprocessing (03_text_preprocessing.ipynb)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}