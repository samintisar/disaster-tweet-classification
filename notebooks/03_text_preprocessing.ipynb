{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Preprocessing\n",
    "\n",
    "This notebook handles advanced text cleaning and preprocessing to prepare tweets for the DistilBERT model.\n",
    "\n",
    "## Preprocessing Steps:\n",
    "1. URL and email removal\n",
    "2. User mention cleaning\n",
    "3. Hashtag processing (#word ‚Üí word)\n",
    "4. Special character and number handling\n",
    "5. Text normalization and cleaning\n",
    "6. Optional spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load configuration\n",
    "with open('../config/hyperparameters.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load preprocessing config\n",
    "preprocess_config = config['feature_config']['text_cleaning']\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Preprocessing config: {preprocess_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Data with Meta-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from previous notebook\n",
    "df_train = pd.read_csv('Data/train_with_meta.csv')\n",
    "df_test = pd.read_csv('Data/test_with_meta.csv')\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "# Verify meta-features are present\n",
    "meta_cols = [col for col in df_train.columns if col.endswith('_meta')]\n",
    "print(f\"\\nMeta-features found: {len(meta_cols)}\")\n",
    "print(f\"First few meta-features: {meta_cols[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text, config):\n",
    "    \"\"\"\n",
    "    Clean tweet text based on configuration.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text)\n",
    "    if config['lowercase']:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    if config['remove_urls']:\n",
    "        text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    if config['remove_emails']:\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove user mentions (@username)\n",
    "    if config['remove_mentions']:\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Convert hashtags to regular words (#word ‚Üí word)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove numbers (except emergency codes like 911)\n",
    "    if config['remove_numbers']:\n",
    "        text = re.sub(r'\\b\\d+\\b(?<!911)', '', text)\n",
    "    \n",
    "    # Remove 'RT' indicators\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def advanced_clean_text(text, config):\n",
    "    \"\"\"\n",
    "    Advanced text cleaning with punctuation and stop word removal.\n",
    "    \"\"\"\n",
    "    # Basic cleaning\n",
    "    text = clean_text(text, config)\n",
    "    \n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # Remove punctuation (keep basic sentence structure)\n",
    "    text = re.sub(r'[^\\w\\s!?]', '', text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    if config['remove_stopwords']:\n",
    "        words = text.split()\n",
    "        words = [w for w in words if w not in stop_words and len(w) > 1]\n",
    "        text = ' '.join(words)\n",
    "    \n",
    "    # Remove character repetition (sooooo ‚Üí so)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "    # Final cleanup\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"‚úÖ Text preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Spell Correction Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to initialize spell correction (optional)\n",
    "spell_corrector = None\n",
    "\n",
    "if preprocess_config['spell_correction']:\n",
    "    try:\n",
    "        from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "        \n",
    "        # Initialize SymSpell\n",
    "        sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "        \n",
    "        # Try to load dictionary\n",
    "        dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
    "        import os\n",
    "        \n",
    "        if os.path.exists(dictionary_path):\n",
    "            sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "            spell_corrector = sym_spell\n",
    "            print(\"‚úÖ Spell correction enabled\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Spell correction dictionary not found, continuing without it\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  SymSpell not available, continuing without spell correction\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Spell correction initialization failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Spell correction disabled in config\")\n",
    "\n",
    "def spell_correct(text, corrector):\n",
    "    \"\"\"\n",
    "    Apply spell correction if available.\n",
    "    \"\"\"\n",
    "    if corrector is None:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # Split text into words for correction\n",
    "        words = text.split()\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            suggestions = corrector.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "            if suggestions:\n",
    "                corrected_words.append(suggestions[0].term)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Spell correction error: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Apply Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned text columns\n",
    "print(\"Applying text preprocessing to training data...\")\n",
    "df_train['text_clean'] = df_train['text'].apply(\n",
    "    lambda x: advanced_clean_text(x, preprocess_config)\n",
    ")\n",
    "\n",
    "print(\"Applying text preprocessing to test data...\")\n",
    "df_test['text_clean'] = df_test['text'].apply(\n",
    "    lambda x: advanced_clean_text(x, preprocess_config)\n",
    ")\n",
    "\n",
    "# Apply spell correction if available\n",
    "if spell_corrector is not None:\n",
    "    print(\"\\nApplying spell correction...\")\n",
    "    df_train['text_clean'] = df_train['text_clean'].apply(\n",
    "        lambda x: spell_correct(x, spell_corrector)\n",
    "    )\n",
    "    df_test['text_clean'] = df_test['text_clean'].apply(\n",
    "        lambda x: spell_correct(x, spell_corrector)\n",
    "    )\n",
    "\n",
    "print(f\"\\nText preprocessing completed!\")\n",
    "print(f\"Train shape after preprocessing: {df_train.shape}\")\n",
    "print(f\"Test shape after preprocessing: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Preprocessing Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze preprocessing results\n",
    "print(\"Preprocessing analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate text length reduction\n",
    "df_train['original_length'] = df_train['text'].str.len()\n",
    "df_train['cleaned_length'] = df_train['text_clean'].str.len()\n",
    "df_train['length_reduction'] = df_train['original_length'] - df_train['cleaned_length']\n",
    "\n",
    "df_test['original_length'] = df_test['text'].str.len()\n",
    "df_test['cleaned_length'] = df_test['text_clean'].str.len()\n",
    "df_test['length_reduction'] = df_test['original_length'] - df_test['cleaned_length']\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(f\"  Average original length: {df_train['original_length'].mean():.1f}\")\n",
    "print(f\"  Average cleaned length: {df_train['cleaned_length'].mean():.1f}\")\n",
    "print(f\"  Average reduction: {df_train['length_reduction'].mean():.1f} characters\")\n",
    "\n",
    "print(\"\\nTest data:\")\n",
    "print(f\"  Average original length: {df_test['original_length'].mean():.1f}\")\n",
    "print(f\"  Average cleaned length: {df_test['cleaned_length'].mean():.1f}\")\n",
    "print(f\"  Average reduction: {df_test['length_reduction'].mean():.1f} characters\")\n",
    "\n",
    "# Check for empty texts after cleaning\n",
    "empty_train = (df_train['text_clean'].str.strip() == '').sum()\n",
    "empty_test = (df_test['text_clean'].str.strip() == '').sum()\n",
    "\n",
    "print(f\"\\nEmpty texts after cleaning:\")\n",
    "print(f\"  Training: {empty_train} ({empty_train/len(df_train)*100:.2f}%)\")\n",
    "print(f\"  Test: {empty_test} ({empty_test/len(df_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Sample Text Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample texts before and after cleaning\n",
    "print(\"Sample texts before and after cleaning:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sample from both classes\n",
    "for target in [0, 1]:\n",
    "    label = 'Non-Disaster' if target == 0 else 'Disaster'\n",
    "    print(f\"\\n{label} Tweets:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    samples = df_train[df_train['target'] == target].head(3)\n",
    "    for idx, row in samples.iterrows():\n",
    "        print(f\"\\nOriginal:  {row['text']}\")\n",
    "        print(f\"Cleaned:   {row['text_clean']}\")\n",
    "        print(f\"Reduction: {row['length_reduction']} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Handle Empty Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle empty texts after cleaning\n",
    "def handle_empty_text(df):\n",
    "    \"\"\"\n",
    "    Replace empty texts with original text as fallback.\n",
    "    \"\"\"\n",
    "    empty_mask = df['text_clean'].str.strip() == ''\n",
    "    \n",
    "    if empty_mask.any():\n",
    "        print(f\"Found {empty_mask.sum()} empty texts, using original text as fallback\")\n",
    "        df.loc[empty_mask, 'text_clean'] = df.loc[empty_mask, 'text']\n",
    "        \n",
    "        # Apply basic cleaning to original text\n",
    "        df.loc[empty_mask, 'text_clean'] = df.loc[empty_mask, 'text_clean'].apply(\n",
    "            lambda x: clean_text(x, preprocess_config)\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Handling empty texts...\")\n",
    "df_train = handle_empty_text(df_train)\n",
    "df_test = handle_empty_text(df_test)\n",
    "\n",
    "# Final check\n",
    "final_empty_train = (df_train['text_clean'].str.strip() == '').sum()\n",
    "final_empty_test = (df_test['text_clean'].str.strip() == '').sum()\n",
    "\n",
    "print(f\"Final empty texts - Train: {final_empty_train}, Test: {final_empty_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Text Length Distribution After Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training data length distribution\n",
    "sns.histplot(data=df_train, x='cleaned_length', hue='target', kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Training Data - Cleaned Text Length Distribution')\n",
    "axes[0, 0].set_xlabel('Character Count')\n",
    "axes[0, 0].legend(['Disaster', 'Non-Disaster'])\n",
    "\n",
    "# Test data length distribution\n",
    "sns.histplot(data=df_test, x='cleaned_length', kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Test Data - Cleaned Text Length Distribution')\n",
    "axes[0, 1].set_xlabel('Character Count')\n",
    "\n",
    "# Length reduction distribution\n",
    "sns.histplot(data=df_train, x='length_reduction', hue='target', kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Character Reduction Distribution')\n",
    "axes[1, 0].set_xlabel('Characters Removed')\n",
    "axes[1, 0].legend(['Disaster', 'Non-Disaster'])\n",
    "\n",
    "# Box plot comparison\n",
    "length_data = pd.DataFrame({\n",
    "    'Original': df_train['original_length'],\n",
    "    'Cleaned': df_train['cleaned_length'],\n",
    "    'Target': df_train['target']\n",
    "})\n",
    "\n",
    "length_melted = length_data.melt(id_vars=['Target'], var_name='Type', value_name='Length')\n",
    "sns.boxplot(data=length_melted, x='Type', y='Length', hue='Target', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Text Length Comparison')\n",
    "axes[1, 1].set_xlabel('Text Type')\n",
    "axes[1, 1].set_ylabel('Character Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Prepare Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select essential columns for model training\n",
    "essential_cols = ['id', 'text', 'text_clean', 'target'] + meta_cols\n",
    "\n",
    "# For test set, target may not exist\n",
    "test_essential_cols = ['id', 'text', 'text_clean'] + meta_cols\n",
    "if 'target' in df_test.columns:\n",
    "    test_essential_cols.append('target')\n",
    "\n",
    "# Create final datasets\n",
    "train_final = df_train[essential_cols].copy()\n",
    "test_final = df_test[test_essential_cols].copy()\n",
    "\n",
    "# Remove temporary columns\n",
    "temp_cols = ['original_length', 'cleaned_length', 'length_reduction']\n",
    "for col in temp_cols:\n",
    "    if col in train_final.columns:\n",
    "        train_final = train_final.drop(col, axis=1)\n",
    "    if col in test_final.columns:\n",
    "        test_final = test_final.drop(col, axis=1)\n",
    "\n",
    "print(f\"Final training dataset shape: {train_final.shape}\")\n",
    "print(f\"Final test dataset shape: {test_final.shape}\")\n",
    "\n",
    "print(\"\\nFinal columns:\")\n",
    "print(f\"Train: {list(train_final.columns)}\")\n",
    "print(f\"Test:  {list(test_final.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned datasets\n",
    "train_final.to_csv('Data/train_cleaned.csv', index=False)\n",
    "test_final.to_csv('Data/test_cleaned.csv', index=False)\n",
    "\n",
    "# Save preprocessing statistics\n",
    "preprocessing_stats = {\n",
    "    'train_samples': len(train_final),\n",
    "    'test_samples': len(test_final),\n",
    "    'avg_original_length': df_train['original_length'].mean(),\n",
    "    'avg_cleaned_length': df_train['cleaned_length'].mean(),\n",
    "    'avg_reduction': df_train['length_reduction'].mean(),\n",
    "    'spell_correction_enabled': spell_corrector is not None,\n",
    "    'preprocessing_config': preprocess_config\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('results/metrics/preprocessing_stats.json', 'w') as f:\n",
    "    json.dump(preprocessing_stats, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Text preprocessing completed!\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"- Data/train_cleaned.csv\")\n",
    "print(\"- Data/test_cleaned.csv\")\n",
    "print(\"- results/metrics/preprocessing_stats.json\")\n",
    "\n",
    "# Display final sample\n",
    "print(\"\\nFinal sample of cleaned training data:\")\n",
    "display(train_final[['text', 'text_clean'] + meta_cols[:3]].head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Text preprocessing completed successfully!\")\n",
    "print(\"Next: Model Training (04_model_training.ipynb)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}