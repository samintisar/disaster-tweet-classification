{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration and Initial Analysis\n",
    "\n",
    "This notebook performs initial data loading, exploration, and visualization of the disaster tweet classification dataset.\n",
    "\n",
    "## Objectives\n",
    "- Load and examine the dataset structure\n",
    "- Perform initial statistical analysis\n",
    "- Visualize data distributions and patterns\n",
    "- Identify data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load configuration\n",
    "with open('../config/hyperparameters.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_path = config['data_paths']['train_raw']\n",
    "test_path = config['data_paths']['test_raw']\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "print(f\"\\nTrain columns: {list(df_train.columns)}\")\n",
    "print(f\"Test columns: {list(df_test.columns)}\")\n",
    "\n",
    "# Memory usage\n",
    "train_memory = df_train.memory_usage().sum() / 1024**2\n",
    "test_memory = df_test.memory_usage().sum() / 1024**2\n",
    "print(f\"\\nTrain memory usage: {train_memory:.2f} MB\")\n",
    "print(f\"Test memory usage: {test_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"Training data sample:\")\n",
    "display(df_train.head())\n",
    "\n",
    "print(\"\\nTest data sample:\")\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"Training data info:\")\n",
    "df_train.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Missing values in train:\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Missing values in test:\")\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = df_train['target'].value_counts()\n",
    "class_percentages = df_train['target'].value_counts(normalize=True) * 100\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar plot\n",
    "class_counts.plot(kind='bar', ax=ax1, color=['lightcoral', 'lightblue'])\n",
    "ax1.set_title('Class Distribution')\n",
    "ax1.set_xlabel('Class')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xticks([0, 1])\n",
    "ax1.set_xticklabels(['Non-Disaster', 'Disaster'])\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(class_counts):\n",
    "    ax1.text(i, count + 50, str(count), ha='center', va='bottom')\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(class_percentages, labels=['Non-Disaster', 'Disaster'], autopct='%1.1f%%',\n",
    "        colors=['lightcoral', 'lightblue'], startangle=90)\n",
    "ax2.set_title('Class Percentage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"Non-Disaster (0): {class_counts[0]} ({class_percentages[0]:.1f}%)\")\n",
    "print(f\"Disaster (1): {class_counts[1]} ({class_percentages[1]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text length columns\n",
    "df_train['text_length'] = df_train['text'].str.len()\n",
    "df_train['word_count'] = df_train['text'].str.split().str.len()\n",
    "\n",
    "df_test['text_length'] = df_test['text'].str.len()\n",
    "df_test['word_count'] = df_test['text'].str.split().str.len()\n",
    "\n",
    "# Text length statistics\n",
    "print(\"Text length statistics (train):\")\n",
    "print(df_train['text_length'].describe())\n",
    "\n",
    "print(\"\\nWord count statistics (train):\")\n",
    "print(df_train['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Text length by class\n",
    "sns.histplot(data=df_train, x='text_length', hue='target', kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Text Length Distribution by Class')\n",
    "axes[0, 0].set_xlabel('Character Count')\n",
    "axes[0, 0].legend(['Disaster', 'Non-Disaster'])\n",
    "\n",
    "# Word count by class\n",
    "sns.histplot(data=df_train, x='word_count', hue='target', kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Word Count Distribution by Class')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].legend(['Disaster', 'Non-Disaster'])\n",
    "\n",
    "# Box plot for text length\n",
    "sns.boxplot(data=df_train, x='target', y='text_length', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Text Length by Class (Box Plot)')\n",
    "axes[1, 0].set_xlabel('Class')\n",
    "axes[1, 0].set_ylabel('Character Count')\n",
    "axes[1, 0].set_xticklabels(['Non-Disaster', 'Disaster'])\n",
    "\n",
    "# Box plot for word count\n",
    "sns.boxplot(data=df_train, x='target', y='word_count', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Word Count by Class (Box Plot)')\n",
    "axes[1, 1].set_xlabel('Class')\n",
    "axes[1, 1].set_ylabel('Word Count')\n",
    "axes[1, 1].set_xticklabels(['Non-Disaster', 'Disaster'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Keyword Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords analysis\n",
    "print(\"Unique keywords in train:\", df_train['keyword'].nunique())\n",
    "print(\"Unique keywords in test:\", df_test['keyword'].nunique())\n",
    "\n",
    "# Top keywords by class\n",
    "disaster_keywords = df_train[df_train['target'] == 1]['keyword'].value_counts().head(20)\n",
    "non_disaster_keywords = df_train[df_train['target'] == 0]['keyword'].value_counts().head(20)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Disaster keywords\n",
    "disaster_keywords.plot(kind='bar', ax=ax1, color='red')\n",
    "ax1.set_title('Top 20 Keywords in Disaster Tweets')\n",
    "ax1.set_xlabel('Keyword')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Non-disaster keywords\n",
    "non_disaster_keywords.plot(kind='bar', ax=ax2, color='blue')\n",
    "ax2.set_title('Top 20 Keywords in Non-Disaster Tweets')\n",
    "ax2.set_xlabel('Keyword')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Location Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location analysis\n",
    "print(\"Unique locations in train:\", df_train['location'].nunique())\n",
    "print(\"Tweets with location info: {df_train['location'].notna().sum() / len(df_train) * 100:.1f}%\")\n",
    "\n",
    "# Top locations\n",
    "top_locations = df_train['location'].value_counts().head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_locations.plot(kind='bar')\n",
    "plt.title('Top 15 Locations')\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Sample Tweets by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample tweets\n",
    "print(\"Sample Disaster Tweets:\")\n",
    "disaster_samples = df_train[df_train['target'] == 1]['text'].head(5)\n",
    "for i, tweet in enumerate(disaster_samples, 1):\n",
    "    print(f\"{i}. {tweet}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Sample Non-Disaster Tweets:\")\n",
    "non_disaster_samples = df_train[df_train['target'] == 0]['text'].head(5)\n",
    "for i, tweet in enumerate(non_disaster_samples, 1):\n",
    "    print(f\"{i}. {tweet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "train_duplicates = df_train.duplicated().sum()\n",
    "test_duplicates = df_test.duplicated().sum()\n",
    "\n",
    "print(f\"Duplicate rows in train: {train_duplicates}\")\n",
    "print(f\"Duplicate rows in test: {test_duplicates}\")\n",
    "\n",
    "# Check for empty texts\n",
    "empty_train = df_train['text'].str.strip().eq('').sum()\n",
    "empty_test = df_test['text'].str.strip().eq('').sum()\n",
    "\n",
    "print(f\"\\nEmpty texts in train: {empty_train}\")\n",
    "print(f\"Empty texts in test: {empty_test}\")\n",
    "\n",
    "# Check for very short texts (< 10 characters)\n",
    "short_train = (df_train['text'].str.len() < 10).sum()\n",
    "short_test = (df_test['text'].str.len() < 10).sum()\n",
    "\n",
    "print(f\"\\nVery short texts (< 10 chars) in train: {short_train}\")\n",
    "print(f\"Very short texts (< 10 chars) in test: {short_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary_stats = {\n",
    "    'Dataset': ['Training', 'Test'],\n",
    "    'Total Samples': [len(df_train), len(df_test)],\n",
    "    'Disaster %': [df_train['target'].mean() * 100, 'N/A'],\n",
    "    'Non-Disaster %': [(1 - df_train['target'].mean()) * 100, 'N/A'],\n",
    "    'Avg Text Length': [df_train['text_length'].mean(), df_test['text_length'].mean()],\n",
    "    'Avg Word Count': [df_train['word_count'].mean(), df_test['word_count'].mean()],\n",
    "    'Unique Keywords': [df_train['keyword'].nunique(), df_test['keyword'].nunique()],\n",
    "    'Unique Locations': [df_train['location'].nunique(), df_test['location'].nunique()]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "print(\"Dataset Summary Statistics:\")\n",
    "display(summary_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Save Enhanced Data\n",
    "\n",
    "Save the enhanced datasets with basic statistics for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced datasets with basic statistics\n",
    "df_train.to_csv('Data/train_enhanced.csv', index=False)\n",
    "df_test.to_csv('Data/test_enhanced.csv', index=False)\n",
    "\n",
    "print(\"âœ… Enhanced datasets saved:\")\n",
    "print(\"- Data/train_enhanced.csv\")\n",
    "print(\"- Data/test_enhanced.csv\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_df.to_csv('results/metrics/data_summary.csv', index=False)\n",
    "print(\"\\nâœ… Summary statistics saved to results/metrics/data_summary.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ Data exploration completed successfully!\")\n",
    "print(\"Next: Feature Engineering (02_feature_engineering.ipynb)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-env)",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
